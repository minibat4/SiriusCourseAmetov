{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import _pickle as cPickle\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. NN training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class SpyHunterDataset(Dataset):\n",
    "    def __init__(self, set_data: pd.DataFrame, file_name: str, mode='train'):\n",
    "        super(SpyHunterDataset, self).__init__()\n",
    "        # 1. Process the data or load already processed.\n",
    "        # 1.1. Group by User and Sort transactions by a datetime. Remove the Datetime column.\n",
    "        # 1.2. Divide into clear and fraud users.\n",
    "        # 1.3. Normalize.\n",
    "        # 1.4. Create helpful arrays for fast indexing (target).\n",
    "        # 1.5. Create helpful arrays for fast indexing (n_transactions).\n",
    "        # 1.6. Dump into the file to reopen fast in the future.\n",
    "        if not os.path.exists(file_name):\n",
    "            # 1.1.\n",
    "            set_data = set_data.sort_values('Datetime')\n",
    "            set_data = set_data[['User', 'Amount']]\n",
    "            # 1.2.\n",
    "            set_data_clear_users = set_data[set_data['IsFraud_target'] == 0]['User'].unique()\n",
    "            set_data_fraud_users = set_data[set_data['IsFraud_target'] == 1]['User'].unique()\n",
    "            # 1.3.\n",
    "            set_data_grouped = set_data.groupby('User')\n",
    "            a_max = set_data_grouped['Amount'].max()\n",
    "            a_min = set_data_grouped['Amount'].min()\n",
    "            for user_num in tqdm(set_data_grouped.groups):\n",
    "                # Normalization is defined for each user individually\n",
    "                l_normalize = lambda x: (x - a_min[user_num]) / (a_max[user_num] - a_min[user_num])\n",
    "                rows = set_data['User'] == user_num\n",
    "                set_data.loc[rows, 'Amount'] = set_data.loc[rows, 'Amount'].apply(l_normalize)\n",
    "            del set_data_grouped\n",
    "            set_data_grouped_normalized = set_data.groupby('User')\n",
    "\n",
    "            # 1.4.\n",
    "            any_user_id_max = set_data['User'].unique().max()\n",
    "            users_array_target = np.zeros(any_user_id_max + 1, dtype=np.int32) - 1\n",
    "            users_array_target[set_data_clear_users] = 0\n",
    "            users_array_target[set_data_fraud_users] = 1\n",
    "            # 1.5.\n",
    "            set_data_users_n_transactions = set_data_grouped_normalized.count()['Amount']\n",
    "            users_array_n_transactions = np.zeros(any_user_id_max + 1, dtype=np.int32) - 1\n",
    "            keys = np.array(set_data_users_n_transactions.keys())\n",
    "            values = np.array(set_data_users_n_transactions.values)\n",
    "            users_array_n_transactions[keys] = values\n",
    "            del set_data_users_n_transactions, keys, values\n",
    "\n",
    "            new_data = [set_data_grouped_normalized,\n",
    "                        set_data_clear_users,\n",
    "                        set_data_fraud_users,\n",
    "                        users_array_target,\n",
    "                        users_array_n_transactions]\n",
    "            with open(file_name, 'wb') as f:\n",
    "                cPickle.dump(new_data, f)\n",
    "        else:\n",
    "            with open(file_name, 'rb') as f:\n",
    "                new_data = cPickle.load(f)\n",
    "\n",
    "        self.set_data_grouped_normalized: pd.DataFrameGroupBy = new_data[0]\n",
    "        self.set_data_clear_users: np.ndarray = new_data[1]\n",
    "        self.set_data_fraud_users: np.ndarray = new_data[2]\n",
    "        self.users_array_target: np.ndarray = new_data[3]\n",
    "        self.users_array_n_transactions: np.ndarray = new_data[4]\n",
    "\n",
    "        # 2. For train set separate Users into clear and fraud subgroups to make them equal in size,\n",
    "        # for valid and test it is not required.\n",
    "        assert mode in ('train', 'eval')\n",
    "        self.chosen_clear = self.set_data_clear_users\n",
    "        self.chosen_fraud = self.set_data_fraud_users\n",
    "        if mode == 'train':\n",
    "            self.rebalance()\n",
    "        # 3. Divide the data into N bins to make sampling, training and finally a convergence faster.\n",
    "        self.chosen_all = np.stack((self.chosen_clear, self.chosen_fraud))\n",
    "        self.order = self.chosen_all\n",
    "        self.reshuflle()\n",
    "        return\n",
    "\n",
    "    def rebalance(self, technique='Downsample'):\n",
    "        # Since clear and fraud users are presented in an unequal amount, for stable binary prediction\n",
    "        # it is required to train in a balanced more. There are some techniques to achieve it.\n",
    "        # 1. Downsample: remove Users from a bigger group. Used by default.\n",
    "        # 2. Upsample: duplicate Users from a smaller group.\n",
    "        # 3. Resample: another technique to upsample a smaller group and downsample a bigger group.\n",
    "        # 4. Other weird sampling techniques.\n",
    "\n",
    "        if technique == 'Downsample':\n",
    "            n_clear = len(self.set_data_clear_users)\n",
    "            n_fraud = len(self.set_data_fraud_users)\n",
    "            if n_clear < n_fraud:\n",
    "                self.chosen_clear = self.set_data_clear_users\n",
    "                self.chosen_fraud = random.sample(population=self.set_data_fraud_users, k=n_clear)\n",
    "            else:\n",
    "                self.chosen_clear = random.sample(population=self.set_data_clear_users, k=n_fraud)\n",
    "                self.chosen_fraud = self.set_data_fraud_users\n",
    "        else:\n",
    "            assert False\n",
    "        return None\n",
    "\n",
    "    def reshuflle(self, n=10):\n",
    "        # Short: divide all the sequences into n bins (define by data analysis or choose any number intuitively).\n",
    "        # 1. Get the values\n",
    "        # 2. Sort users according to these values, sort values also.\n",
    "        # 3. Sort and divide the array into n equal bins.\n",
    "        # 4. Shuffle users in each bin.\n",
    "        # Comments\n",
    "        # - Smaller n - more rough, may be useful during training to increase a diversity\n",
    "        # of samples in a batch. Especially useful when batch size is smaller, but may reduce\n",
    "        # the computation gain. For big batches, shuffling will not increase diversity since samples may\n",
    "        # continue to be in one pool.\n",
    "        # - Higher n - better accuracy estimation, may be useful during validation\n",
    "        # since the impact of zeros at the start is reduced\n",
    "        values = self.users_array_n_transactions[self.chosen_all]\n",
    "        indexes = np.argsort(values)\n",
    "        self.order = self.chosen_all[indexes]\n",
    "        assert n < self.chosen_all\n",
    "        nth_path = len(self.chosen_all) // n\n",
    "        print('Shuffling statistics')\n",
    "        for i in range(n):\n",
    "            left = i * nth_path\n",
    "            right = min(len(self.chosen_all), (i + 1) * nth_path)\n",
    "            print(f'--> Group {i:>2} '\n",
    "                  f'MIN {self.users_array_n_transactions[self.chosen_all[left]]:>6} '\n",
    "                  f'MAX {self.users_array_n_transactions[self.chosen_all[right]]:>6}')\n",
    "            random.shuffle(self.order[left:right])\n",
    "        print('Finished shuffling')\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.order)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Return a sequence for a User\n",
    "        user_index = self.order[item]\n",
    "        sequence = self.set_data_grouped_normalized.get_group(user_index)\n",
    "        # An idea proposed by a the teacher is to reduce the sequence, but it is not clear how.\n",
    "        return sequence, len(sequence), self.users_array_target[user_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def transactions_collate_fn(data):\n",
    "    # 1. Batch randomization is enabled with Dataset data ordering.\n",
    "    # 2. Pad all the sequences up to longest sequence length in the batch.\n",
    "    # 2.1. Why to pad in this method: https://discuss.pytorch.org/t/how-to-use-collate-fn/27181/4\n",
    "    # 2.2. Left zero padding is preferred over right zero padding to prevent gradient vanishing,\n",
    "    # seems to be equally important to the problem described in https://arxiv.org/abs/1409.3215\n",
    "    # Comments\n",
    "    # Another example\n",
    "    # https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders\n",
    "\n",
    "    sequences, lengths, labels = zip(*data)\n",
    "    # Do not know if these arrays are numpy arrays\n",
    "    max_len = max(lengths)\n",
    "    sequences_new = torch.zeros((len(data), max_len))\n",
    "    labels = torch.tensor(lengths)\n",
    "    for i in range(len(data)):\n",
    "        index = max_len - len(sequences[i])\n",
    "        sequences_new[index:] = sequences[i]\n",
    "    return sequences_new, labels\n",
    "\n",
    "\n",
    "def worker_init_fn(id):\n",
    "    # https://github.com/pytorch/pytorch/issues/5059#issuecomment-624788105\n",
    "    print(f'Initialize a worker #{id:<2}', torch.initial_seed(), flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataloader - simple\n",
    "# tensorboard\n",
    "# neural network - LSTM with several layers and FC in the end\n",
    "\n",
    "# train loop\n",
    "# valid loop\n",
    "# test loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "with open('../data/transactions_train_valid_test_splits_postprocessed.pickle', 'rb') as f:\n",
    "    data = cPickle.load(f)\n",
    "\n",
    "train_data = data['train']\n",
    "valid_date = data['valid']\n",
    "test_data = data['test']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SpyHunterDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_3232/1855054268.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSpyHunterDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mset_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfile_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'../data/train_dataset_data.pkl'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'SpyHunterDataset' is not defined"
     ]
    }
   ],
   "source": [
    "ds = SpyHunterDataset(set_data=train_data, file_name='../data/train_dataset_data.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (223719256.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"/tmp/ipykernel_3232/223719256.py\"\u001B[0;36m, line \u001B[0;32m3\u001B[0m\n\u001B[0;31m    class MyTrainer:\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class MyRNNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRNNNet, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "class MyTrainer:\n",
    "    overfit_one_batch = False\n",
    "\n",
    "    d_model = 512\n",
    "    heads = 8\n",
    "\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.98\n",
    "    eps = 10e-9\n",
    "\n",
    "    lr = 0.01\n",
    "    weight_decay = 0.0001\n",
    "\n",
    "    warmup_steps = 4000\n",
    "    # LR Adjust\n",
    "    coeffs1 = 1 / (d_model ** 0.5)\n",
    "    coeffs2 = 1 / (warmup_steps ** 1.5)\n",
    "\n",
    "    global_step = 1\n",
    "    check_step = False\n",
    "\n",
    "    epochs = 100\n",
    "    num_workers = 4\n",
    "    pin_memory = True\n",
    "    shuffle = False\n",
    "    train_bsize = 128\n",
    "    eval_bsize = 64\n",
    "    device = 'cuda:0'\n",
    "\n",
    "    initial_epoch = -1\n",
    "    best_acc_value = -1\n",
    "    best_acc_epoch = -1\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rnn = MyRNNNet()\n",
    "        self.opt = torch.optim.Adam(self.rnn.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        \n",
    "        self.valid_ds = SpyHunterDataset(valid_date, file_name='valid_dataset_data.pkl')\n",
    "\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        short = datetime.now().__str__()[:-7].replace(\":\", \"-\")\n",
    "        self.log_dir = f'models/Run {short}'\n",
    "        assert not os.path.exists(self.log_dir)\n",
    "\n",
    "        self.tXw = SummaryWriter(self.log_dir)\n",
    "        return\n",
    "\n",
    "    def train(self):\n",
    "        train_ds = SpyHunterDataset(train_data, file_name='train_dataset_data.pkl')\n",
    "        train_dloader = DataLoader(train_ds, batch_size=self.eval_bsize, shuffle=False, num_workers=1,\n",
    "                             collate_fn=transactions_collate_fn, pin_memory=True, drop_last=False)\n",
    "        valid_ds = SpyHunterDataset(valid_date, file_name='valid_dataset_data.pkl')\n",
    "        \n",
    "        for data in tqdm(dloader):\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "            sequences, labels = data\n",
    "            sequences = sequences.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "\n",
    "            outputs = self.rnn(sequences)\n",
    "            loss = self.loss_fn(outputs, target=labels)\n",
    "\n",
    "            batch_balanced_acc = balanced_accuracy_score(y_true=labels.detach().numpy(),\n",
    "                                                         y_pred=outputs.detach().numpy())\n",
    "            self.tXw.add_scalar('Train/BatchLoss', loss.item(), self.global_step)\n",
    "            self.tXw.add_scalar('Train/BalancedAcc', batch_balanced_acc, self.global_step)\n",
    "            self.global_step += 1\n",
    "\n",
    "        return\n",
    "\n",
    "    def eval(self, eval_ds):\n",
    "        # dloader = DataLoader(eval_ds, batch_size=self.eval_bsize, shuffle=False, num_workers=1,\n",
    "        #                      collate_fn=transactions_collate_fn, pin_memory=True, drop_last=False)\n",
    "        # for data in tqdm(dloader):\n",
    "        #     self.opt.zero_grad()\n",
    "        #\n",
    "        #     sequences, labels = data\n",
    "        #     sequences = sequences.to('cuda')\n",
    "        #     labels = labels.to('cuda')\n",
    "        #\n",
    "        #     output = self.rnn(sequences)\n",
    "        #     loss = self.loss_fn(output, target=labels)\n",
    "        #\n",
    "        return\n",
    "\n",
    "    def test(self):\n",
    "        self.test_ds = SpyHunterDataset(test_data, file_name='test_dataset_data.pkl')\n",
    "        self.eval(self.test_ds)\n",
    "\n",
    "\n",
    "    def save(self, path):\n",
    "        chkpt = {\n",
    "            'rnn': self.rnn.state_dict(),\n",
    "            'opt': self.opt.state_dict(),\n",
    "            'global_step': self.global_step,\n",
    "            'best_acc_value': self.best_acc_value,\n",
    "            'best_acc_epoch': self.best_acc_epoch\n",
    "\n",
    "        }\n",
    "\n",
    "        torch.save(chkpt, path)\n",
    "        print('Saved a model to', path)\n",
    "        return\n",
    "\n",
    "    def load(self, path):\n",
    "        chkpt = torch.load(path)\n",
    "\n",
    "        self.rnn.load_state_dict(chkpt['transformer'])\n",
    "        self.opt.load_state_dict(chkpt['opt'])\n",
    "        self.global_step = chkpt['global_step']\n",
    "        self.best_acc_value = chkpt['best_acc_value']\n",
    "        self.best_acc_epoch = chkpt['best_acc_epoch']\n",
    "\n",
    "        print('Loaded a model from', path)\n",
    "        return\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}