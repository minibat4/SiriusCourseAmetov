{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "from plotly import graph_objects as go"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. NN training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (592525440.py, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"/tmp/ipykernel_22043/592525440.py\"\u001B[0;36m, line \u001B[0;32m46\u001B[0m\n\u001B[0;31m    else:\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class SpyHunterDataset(Dataset):\n",
    "    def __init__(self, set_data: pd.DataFrame, file_name: str, mode='train'):\n",
    "        super(SpyHunterDataset, self).__init__()\n",
    "        # 1. Process the data or load already processed.\n",
    "        # 1.1. Group by User and Sort transactions by a datetime.\n",
    "        # 1.2. Drop datetime since the data is ordered. Priority to the memory consumption.\n",
    "        if not os.path.exists(file_name):\n",
    "            # Sort by datetime\n",
    "            set_data_grouped = set_data.sort_values('Datetime')\n",
    "            # Remove the column just used by not choosing it\n",
    "            set_data_grouped = set_data_grouped[['User', 'Amount']]\n",
    "            # Group by User\n",
    "            set_data_grouped = set_data_grouped.groupby('User')\n",
    "            # Get labels in nd.arrays\n",
    "            set_date_clear_users = set_data[set_data['IsFraud_target'] == 0]['User'].unique()\n",
    "            set_data_fraud_users = set_data[set_data['IsFraud_target'] == 1]['User'].unique()\n",
    "            new_data = [set_data_grouped, set_date_clear_users, set_data_fraud_users]\n",
    "            with open(file_name, 'wb') as f:\n",
    "                cPickle.dump(new_data, f)\n",
    "        else:\n",
    "            with open(file_name, 'rb') as f:\n",
    "                new_data = cPickle.load(f)\n",
    "\n",
    "        self.set_data_grouped = new_data[0]\n",
    "        self.set_date_clear_users = new_data[1]\n",
    "        self.set_data_fraud_users = new_data[2]\n",
    "\n",
    "        # 2. For train set separate Users into clear and fraud subgroups to make them equal in size,\n",
    "        # for valid and test it is not required since all the users have to be classified\n",
    "        # 2.1. If train, balance the clear and fraud subgroups. More detail in the self.rebalance() method.\n",
    "        assert mode in ('train', 'eval')\n",
    "        self.chosen_clear = self.set_date_clear_users\n",
    "        self.chosen_fraud = self.set_data_fraud_users\n",
    "        if mode == 'train':\n",
    "            self.rebalance()\n",
    "        # 3. Divide the data into N bins to make sampling, training and finally a convergence faster.\n",
    "        # More details in the self.reshuflle() method.\n",
    "\n",
    "\n",
    "    def rebalance(self, technique='Downsample'):\n",
    "        # Since clear and fraud users are presented in an unequal amount, for stable binary prediction\n",
    "        # it is required to train in a balanced more. There are some techniques to achieve it.\n",
    "        # 1. Downsample: remove Users from a bigger group. Used by default.\n",
    "        # 2. Upsample: duplicate Users from a smaller group.\n",
    "        # 3. Resample: another technique to upsample a smaller group and downsample a bigger group.\n",
    "        # 4. Other weird sampling techniques.\n",
    "\n",
    "        if technique == 'Downsample':\n",
    "            n_clear = len(self.set_date_clear_users)\n",
    "            n_fraud = len(self.set_data_fraud_users)\n",
    "            if n_clear < n_fraud:\n",
    "                self.chosen_clear = self.set_data_fraud_users\n",
    "                self.chosen_fraud = random.sample(population=self.set_data_fraud_users, k=n_clear)\n",
    "            else:\n",
    "                self.chosen_fraud = self.set_data_fraud_users\n",
    "                self.chosen_clear = random.sample(population=self.set_date_clear_users, k=n_fraud)\n",
    "        else:\n",
    "            assert False\n",
    "        return None\n",
    "\n",
    "    def reshuflle(self):\n",
    "        # Divide all the sequences into N bins (define by data analysis or choose any number intuitively).\n",
    "        # Smaller N - more rough, may be useful during training to increase a diversity\n",
    "        # of samples in a batch. Especially useful when batch size is smaller, but may reduce\n",
    "        # the computation gain. For big batches, shuffling will not increase diversity since samples may\n",
    "        # continue to be in one pool.\n",
    "        # Higher N - better accuracy estimation, may be useful during validation\n",
    "        # since the impact of zeros at the start is reduced\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.groups)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Return a sequence for a User under\n",
    "        return self.groups['item']\n",
    "\n",
    "\n",
    "def collate_fn():\n",
    "    # 1. Handle randomization for dataloaders.\n",
    "    # 2. Pad all the sequences up to len of the longest sequence in the batch.\n",
    "    # 2.1. Why to pad in this method: https://discuss.pytorch.org/t/how-to-use-collate-fn/27181/4\n",
    "    # 2.2. Left zero padding is preferred over right zero padding to prevent gradient vanishing,\n",
    "    # seems to be equally important to the problem described in https://arxiv.org/abs/1409.3215\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataset - padded sequences for all the users to allow fast batching, use collate_fn\n",
    "# (first time in my practice)\n",
    "\n",
    "# Varying batch length\n",
    "# dataloader - simple\n",
    "# tensorboard\n",
    "# neural network - LSTM with several layers and FC in the end\n",
    "\n",
    "# train loop\n",
    "# valid loop\n",
    "# test loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with open('transactions_train_valid_test_splits_postprocessed.pickle', 'rb') as f:\n",
    "    data = cPickle.load(f)\n",
    "\n",
    "train_data = data['train']\n",
    "valid_date = data['valid']\n",
    "test_data = data['test']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}